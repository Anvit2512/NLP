{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebeb478e-8d87-43f6-864b-30d8d4953c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cbfd0bc-e186-4e1d-af46-c6f2b81f58b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "Strange\n",
      "loves\n",
      "pav\n",
      "bhaji\n",
      "of\n",
      "mumbai\n",
      "as\n",
      "it\n",
      "costs\n",
      "only\n",
      "2\n",
      "$\n",
      "per\n",
      "plate\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "nlp=spacy.blank(\"en\")\n",
    "doc=nlp(\"Dr. Strange loves pav bhaji of mumbai as it costs only 2$ per plate.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "429b3d16-b2ee-47f0-90b0-a42e58634796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e67b7467-c0c6-430e-98e8-e05690c54b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1129bea-ff13-4fd0-bedb-b4087836ed12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed4dbbef-060f-43bd-8937-55b5d23b8aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(\"Tony gave two $ to peter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "243bb31a-4af1-4c1d-bcc7-787b6d023741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tony"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0=doc[0]\n",
    "token0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c43c6f08-0060-44b0-a1ac-d1dbd44c89cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1461654d-ca35-4c15-9edb-684c7e0454f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_dep',\n",
       " 'has_extension',\n",
       " 'has_head',\n",
       " 'has_morph',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'iob_strings',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'set_morph',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(token0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab680848-cc75-4b95-94ad-c49c3f29acab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0.is_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bab0e2ca-e054-44f4-8829-fd2a19270d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tony ==> index:  0 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "gave ==> index:  1 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "two ==> index:  2 is_alpha: True is_punct: False like_num: True is_currency: False\n",
      "$ ==> index:  3 is_alpha: False is_punct: False like_num: False is_currency: True\n",
      "to ==> index:  4 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "peter ==> index:  5 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      ". ==> index:  6 is_alpha: False is_punct: True like_num: False is_currency: False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, \"==>\", \"index: \", token.i, \"is_alpha:\", token.is_alpha, \n",
    "          \"is_punct:\", token.is_punct, \n",
    "          \"like_num:\", token.like_num,\n",
    "          \"is_currency:\", token.is_currency,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77466d62-a744-4f33-bacb-9c2b9bdda3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John Doe;johndoe@gmail.com;01/15/2000\\n',\n",
       " 'Jane Smith;janesmith@gmail.com;04/22/1998\\n',\n",
       " 'Robert Brown;robertbrown@yahoo.com;09/30/2001\\n',\n",
       " 'Emily White;emilywhite@hotmail.com;06/17/1999\\n',\n",
       " 'Michael Johnson;michaeljohnson@outlook.com;02/04/2000\\n']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"student_list.txt\") as f:\n",
    "    text = f.readlines()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f58ff3d-02c9-42ae-875d-a1ce4b19723c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'John Doe;johndoe@gmail.com;01/15/2000\\n Jane Smith;janesmith@gmail.com;04/22/1998\\n Robert Brown;robertbrown@yahoo.com;09/30/2001\\n Emily White;emilywhite@hotmail.com;06/17/1999\\n Michael Johnson;michaeljohnson@outlook.com;02/04/2000\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text= ' '.join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c584347-2e8e-4b92-ab33-e498c6757a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=nlp(text)\n",
    "emails=[]\n",
    "for token in doc:\n",
    "    if token.like_email:\n",
    "        emails.append(token.text)\n",
    "emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f215bd6a-b3b5-47d6-a8df-81010a74f06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['johndoe@gmail.com', 'janesmith@yahoo.com', 'emilywhite@hotmail.com']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text containing emails\n",
    "text = \"Here are some emails: johndoe@gmail.com, janesmith@yahoo.com, emilywhite@hotmail.com\"\n",
    "\n",
    "# Process the text with the NLP pipeline\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract emails\n",
    "emails = [token.text for token in doc if token.like_email]\n",
    "\n",
    "# Output the emails\n",
    "print(emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10e3efde-b96b-4061-ab50-25e23082b229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "file_path = \"student_list.txt\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Process the text with the NLP pipeline\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract emails\n",
    "emails = [token.text for token in doc if token.like_email]\n",
    "\n",
    "# Output the emails\n",
    "print(emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61fef829-d712-423d-b168-ee841f505d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "भारत False False\n",
      "एक False True\n",
      "विविधता False False\n",
      "से False False\n",
      "भरा False False\n",
      "हुआ False False\n",
      "देश False False\n",
      "है False False\n",
      ", False False\n",
      "जहाँ False False\n",
      "विभिन्न False False\n",
      "संस्कृतियाँ False False\n",
      ", False False\n",
      "भाषाएँ False False\n",
      ", False False\n",
      "और False False\n",
      "परंपराएँ False False\n",
      "पाई False False\n",
      "जाती False False\n",
      "हैं False False\n",
      "। False False\n",
      "यहाँ False False\n",
      "की False False\n",
      "सांस्कृतिक False False\n",
      "धरोहर False False\n",
      "और False False\n",
      "ऐतिहासिक False False\n",
      "महत्व False False\n",
      "उसे False False\n",
      "एक False True\n",
      "अद्वितीय False False\n",
      "स्थान False False\n",
      "प्रदान False False\n",
      "करते False False\n",
      "हैं False False\n",
      "। False False\n",
      "भारतीय False False\n",
      "समाज False False\n",
      "में False False\n",
      "भाईचारे False False\n",
      "और False False\n",
      "सद्भाव False False\n",
      "का False False\n",
      "विशेष False False\n",
      "स्थान False False\n",
      "है False False\n",
      "। False False\n",
      "यहाँ False False\n",
      "के False False\n",
      "लोग False False\n",
      "अपनी False False\n",
      "विविधता False False\n",
      "में False False\n",
      "एकता False False\n",
      "को False False\n",
      "महत्व False False\n",
      "देते False False\n",
      "हैं False False\n",
      "और False False\n",
      "हर False False\n",
      "एक False True\n",
      "त्योहार False False\n",
      "और False False\n",
      "अवसर False False\n",
      "को False False\n",
      "मिलजुल False False\n",
      "कर False False\n",
      "मनाते False False\n",
      "हैं False False\n",
      "। False False\n",
      "भारतीय False False\n",
      "खाना False False\n",
      ", False False\n",
      "संगीत False False\n",
      ", False False\n",
      "कला False False\n",
      "और False False\n",
      "नृत्य False False\n",
      "विश्वभर False False\n",
      "में False False\n",
      "प्रसिद्ध False False\n",
      "हैं False False\n",
      "। False False\n",
      "भारत False False\n",
      "की False False\n",
      "धरती False False\n",
      "पर False False\n",
      "अनेक False False\n",
      "धार्मिक False False\n",
      "स्थल False False\n",
      "और False False\n",
      "प्राचीन False False\n",
      "किलों False False\n",
      "का False False\n",
      "इतिहास False False\n",
      "समाया False False\n",
      "हुआ False False\n",
      "है False False\n",
      ", False False\n",
      "जो False False\n",
      "हमारे False False\n",
      "गौरवपूर्ण False False\n",
      "अतीत False False\n",
      "की False False\n",
      "गवाही False False\n",
      "देते False False\n",
      "हैं False False\n",
      "। False False\n"
     ]
    }
   ],
   "source": [
    "nlp=spacy.blank(\"hi\")\n",
    "doc=nlp(\"भारत एक विविधता से भरा हुआ देश है, जहाँ विभिन्न संस्कृतियाँ, भाषाएँ, और परंपराएँ पाई जाती हैं। यहाँ की सांस्कृतिक धरोहर और ऐतिहासिक महत्व उसे एक अद्वितीय स्थान प्रदान करते हैं। भारतीय समाज में भाईचारे और सद्भाव का विशेष स्थान है। यहाँ के लोग अपनी विविधता में एकता को महत्व देते हैं और हर एक त्योहार और अवसर को मिलजुल कर मनाते हैं। भारतीय खाना, संगीत, कला और नृत्य विश्वभर में प्रसिद्ध हैं। भारत की धरती पर अनेक धार्मिक स्थल और प्राचीन किलों का इतिहास समाया हुआ है, जो हमारे गौरवपूर्ण अतीत की गवाही देते हैं।\")\n",
    "for token in doc:\n",
    "    print(token, token.is_currency, token.like_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83684da1-a308-47d9-aa04-c3bedf9aab66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gimme', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=nlp(\"gimme double cheese extra large healthy pizza\")\n",
    "\n",
    "tokens=[token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25bec355-5f6b-4e7f-b469-a131557897aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gim', 'me', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.symbols import ORTH\n",
    "nlp.tokenizer.add_special_case(\"gimme\",[{ORTH:\"gim\"},{ORTH:\"me\"}])\n",
    "\n",
    "doc=nlp(\"gimme double cheese extra large healthy pizza\")\n",
    "\n",
    "tokens=[token.text for token in doc]\n",
    "tokens                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7519ebb0-7895-40e3-a0c7-199da2a0b946",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m doc\u001b[38;5;241m=\u001b[39mnlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msents:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(sentence)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\spacy_env\\lib\\site-packages\\spacy\\tokens\\doc.pyx:926\u001b[0m, in \u001b[0;36msents\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`."
     ]
    }
   ],
   "source": [
    "doc=nlp(\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\")\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1c69fbd-109e-4371-92ee-23b330fd34a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x1f7d17958c0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ee772cf-717d-413b-8c73-134caf384a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentencizer']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2dc171d-c158-408d-8be2-4f5317d6c0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "Strange loves pav bhaji of mumbai.\n",
      "Hulk loves chat of delhi\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\")\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd03e146-72b4-4260-9654-25964e2c9f76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
